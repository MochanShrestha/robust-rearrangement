actor_name: residual_diffusion

seed: null
torch_deterministic: false
headless: true
reset_on_failure: false
reset_every_iteration: true
reset_on_success: false
eval_interval: 5
eval_first: true
checkpoint_interval: 1
truncation_as_done: true
base_policy: 
  wandb_id: ol-state-dr-1/r9wm1uo6

moidels_we_depend_on:
total_timesteps: 500_000_000
num_envs: 1024
num_minibatches: 1
update_epochs: 50
num_env_steps: 700
data_collection_steps: ${num_env_steps}

observation_type: state

env:
  randomness: med

residual_policy:
  _target_: src.models.residual.ResidualPolicy
  init_logstd: -1.5
  action_head_std: 0.0
  action_scale: 0.1
  actor_hidden_size: 256
  actor_num_layers: 2
  critic_hidden_size: 256
  critic_num_layers: 2
  actor_activation: ReLU
  critic_activation: ${residual_policy.actor_activation}
  critic_last_layer_bias_const: 0.25
  critic_last_layer_std: 0.25
  critic_last_layer_activation: null

  # pretrained_wts: /data/scratch/ankile/robust-rearrangement/models/1716310794__residual_ppo__ResidualPolicy__907769811/iter_520.pt
  pretrained_wts: null

# Learning rates
learning_rate_actor: 3e-4
learning_rate_critic: 5e-3
lr_scheduler:
  name: cosine
  warmup_steps: 8

# Residual specific arguments
residual_l1: 0.0
residual_l2: 0.01

# Algorithm specific arguments
gamma: 0.999
gae_lambda: 0.95
norm_adv: true
normalize_reward: true
clip_reward: 5.0
clip_coef: 0.2
clip_vloss: false
ent_coef: 0.0
vf_coef: 1.0
max_grad_norm: 1.0
target_kl: 0.1
n_iterations_train_only_value: 0

# Calculate the batch size as the data collection steps times the number of environments
batch_size: ${eval:'${data_collection_steps} * ${num_envs}'}
minibatch_size: ${eval:'${batch_size} // ${num_minibatches}'}
num_iterations: ${eval:'${total_timesteps} // ${batch_size}'}

action_type: pos
act_rot_repr: rot_6d

wandb:
  entity: robust-rearrangement
  project: residual-ppo-dr-debug-1
  mode: online

debug: false
