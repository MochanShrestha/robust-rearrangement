{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from src.behavior import get_actor\n",
    "from src.eval.eval_utils import get_model_from_api_or_cached\n",
    "\n",
    "\n",
    "from src.common.files import get_processed_paths, path_override\n",
    "from torch.utils.data import DataLoader\n",
    "from src.dataset.dataset import FurnitureImageDataset\n",
    "from src.train.bc import to_native\n",
    "\n",
    "import torch\n",
    "from src.behavior.base import Actor\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id_naive = \"real-ol-demo-scaling-1/3js1f6n1\"\n",
    "run_id_upwt = \"real-ol-demo-scaling-1/31xxjkpb\"\n",
    "run_id_conf = \"real-ol-demo-scaling-1/1knzc1b4\"\n",
    "run_id_confusion4 = \"real-one_leg-cotrain-2/7grrzinv\"\n",
    "run_id_confusion3 = \"real-one_leg-cotrain-2/xwawbdtk\"\n",
    "run_id_confusion2 = \"real-one_leg-cotrain-2/f7usetuv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the config to load in the standard model with only pretrained weights\n",
    "cfg, _ = get_model_from_api_or_cached(run_id_naive, \"latest\", wandb_mode=\"online\")\n",
    "\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor: Actor = get_actor(cfg=cfg, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off the actor's training mode and gradient computation\n",
    "actor.eval()\n",
    "\n",
    "for param in actor.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor.model.training, actor.encoder1.training, actor.encoder2.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.data.data_paths_override is None:\n",
    "    data_path = get_processed_paths(\n",
    "        controller=to_native(cfg.control.controller),\n",
    "        domain=to_native(cfg.data.environment),\n",
    "        task=to_native(cfg.data.furniture),\n",
    "        demo_source=to_native(cfg.data.demo_source),\n",
    "        randomness=to_native(cfg.data.randomness),\n",
    "        demo_outcome=to_native(cfg.data.demo_outcome),\n",
    "        suffix=to_native(cfg.data.suffix),\n",
    "    )\n",
    "else:\n",
    "    data_path = path_override(cfg.data.data_paths_override)\n",
    "\n",
    "print(f\"Using data from {data_path}\")\n",
    "\n",
    "dataset = FurnitureImageDataset(\n",
    "    dataset_paths=data_path,\n",
    "    pred_horizon=cfg.data.pred_horizon,\n",
    "    obs_horizon=cfg.data.obs_horizon,\n",
    "    action_horizon=cfg.data.action_horizon,\n",
    "    # data_subset=cfg.data.data_subset,\n",
    "    data_subset=5,\n",
    "    control_mode=cfg.control.control_mode,\n",
    "    predict_past_actions=cfg.data.predict_past_actions,\n",
    "    pad_after=cfg.data.get(\"pad_after\", True),\n",
    "    max_episode_count=cfg.data.get(\"max_episode_count\", None),\n",
    "    minority_class_power=cfg.data.get(\"minority_class_power\", False),\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "trainload_kwargs = dict(\n",
    "    dataset=dataset,\n",
    "    # batch_size=cfg.training.batch_size,\n",
    "    batch_size=64,\n",
    "    num_workers=cfg.data.dataloader_workers,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    "    persistent_workers=False,\n",
    ")\n",
    "\n",
    "trainloader = DataLoader(**trainload_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(actor: Actor, batch):\n",
    "    img1 = batch[\"color_image1\"].to(\"cuda\").squeeze()\n",
    "    emb1 = actor.encoder1_proj(actor.encoder1(img1))\n",
    "\n",
    "    img2 = batch[\"color_image2\"].to(\"cuda\").squeeze()\n",
    "    emb2 = actor.encoder2_proj(actor.encoder2(img2))\n",
    "\n",
    "    return emb1, emb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_and_domain_labels(actor: Actor, trainloader, sample_size=None):\n",
    "    # Initialize empty lists to store embeddings and domain labels\n",
    "    embeddings = []\n",
    "    domain_labels = []\n",
    "\n",
    "    # Iterate over the dataset\n",
    "    for batch in tqdm(trainloader):\n",
    "        emb1, emb2 = get_embeddings(actor, batch)\n",
    "\n",
    "        # Concatenate the embeddings into a single tensor of shape (batch_size, 2 * embedding_size)\n",
    "        emb = torch.cat([emb1, emb2], dim=1)\n",
    "        embeddings.append(emb.cpu().numpy())\n",
    "\n",
    "        domain_labels.extend(batch[\"domain\"].cpu().numpy().tolist())\n",
    "\n",
    "    # Concatenate the embeddings and convert to numpy array\n",
    "    embeddings = np.concatenate(embeddings, axis=0)\n",
    "    domain_labels = np.array(domain_labels).reshape(-1)\n",
    "\n",
    "    # Print the average standard deviation of the embeddings\n",
    "    print(\n",
    "        f\"Average standard deviation of embeddings: {np.mean(np.std(embeddings, axis=0))}\"\n",
    "    )\n",
    "\n",
    "    # If sample_size is not None, sample a subset of the embeddings and domain labels\n",
    "    # Stratified by domain label\n",
    "    if sample_size is not None:\n",
    "        sampled_embeddings = []\n",
    "        sampled_domain_labels = []\n",
    "\n",
    "        for domain_label in np.unique(domain_labels):\n",
    "            idx = np.where(domain_labels == domain_label)[0]\n",
    "            idx = np.random.choice(idx, size=sample_size, replace=False)\n",
    "\n",
    "            sampled_embeddings.append(embeddings[idx])\n",
    "            sampled_domain_labels.extend(domain_labels[idx].tolist())\n",
    "\n",
    "        embeddings = np.concatenate(sampled_embeddings, axis=0)\n",
    "        domain_labels = np.array(sampled_domain_labels).reshape(-1)\n",
    "\n",
    "    return embeddings, domain_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simcolor = \"#2398DA\"\n",
    "realcolor = \"#E34A6F\"\n",
    "\n",
    "\n",
    "def visualize_embeddings_tsne(embeddings, domain_labels, title=None):\n",
    "    # Apply t-SNE to reduce the dimensionality to 2\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    embeddings_tsne = tsne.fit_transform(embeddings)\n",
    "\n",
    "    # Split the embeddings based on the domain labels\n",
    "    embeddings_domain1 = embeddings_tsne[domain_labels == 0]\n",
    "    embeddings_domain2 = embeddings_tsne[domain_labels == 1]\n",
    "\n",
    "    # Plot the embeddings in two different colors\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.scatter(\n",
    "        embeddings_domain1[:, 0],\n",
    "        embeddings_domain1[:, 1],\n",
    "        color=simcolor,\n",
    "        label=\"Sim\",\n",
    "        alpha=0.2,\n",
    "        s=2,\n",
    "    )\n",
    "    plt.scatter(\n",
    "        embeddings_domain2[:, 0],\n",
    "        embeddings_domain2[:, 1],\n",
    "        color=realcolor,\n",
    "        label=\"Real\",\n",
    "        alpha=0.2,\n",
    "        s=2,\n",
    "    )\n",
    "    # plt.xlabel('t-SNE Dimension 1')\n",
    "    # plt.ylabel('t-SNE Dimension 2')\n",
    "    plt.legend(frameon=False)\n",
    "\n",
    "    if title is not None:\n",
    "        plt.title(title + \" (t-SNE)\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_embeddings_pca(embeddings, domain_labels, title=None):\n",
    "    # Apply PCA to reduce the dimensionality to 2\n",
    "    pca = PCA(n_components=2)\n",
    "    embeddings_pca = pca.fit_transform(embeddings)\n",
    "\n",
    "    # Split the embeddings based on the domain labels\n",
    "    embeddings_domain1 = embeddings_pca[domain_labels == 0]\n",
    "    embeddings_domain2 = embeddings_pca[domain_labels == 1]\n",
    "\n",
    "    # Plot the embeddings in two different colors\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.scatter(\n",
    "        embeddings_domain1[:, 0],\n",
    "        embeddings_domain1[:, 1],\n",
    "        color=simcolor,\n",
    "        label=\"Sim\",\n",
    "        alpha=0.2,\n",
    "        s=2,\n",
    "    )\n",
    "    plt.scatter(\n",
    "        embeddings_domain2[:, 0],\n",
    "        embeddings_domain2[:, 1],\n",
    "        color=realcolor,\n",
    "        label=\"Real\",\n",
    "        alpha=0.2,\n",
    "        s=2,\n",
    "    )\n",
    "    # plt.xlabel('PCA Dimension 1')\n",
    "    # plt.ylabel('PCA Dimension 2')\n",
    "    plt.legend(frameon=False)\n",
    "\n",
    "    if title is not None:\n",
    "        plt.title(title + \" (PCA)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch3d.transforms import so3_exponential_map, so3_relative_angle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot embeddings for the pretrained R3M model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, domain_labels = get_embeddings_and_domain_labels(\n",
    "    actor, trainloader, sample_size=sample_size\n",
    ")\n",
    "\n",
    "title = \"Pre-trained R3M weights\"\n",
    "visualize_embeddings_tsne(embeddings, domain_labels, title=title)\n",
    "visualize_embeddings_pca(embeddings, domain_labels, title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot embeddings for co-trained model with no tricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get weights\n",
    "_, wts = get_model_from_api_or_cached(run_id_naive, \"latest\", wandb_mode=\"online\")\n",
    "\n",
    "# Load the weights into the actor\n",
    "state_dict = torch.load(wts)\n",
    "if \"model_state_dict\" in state_dict:\n",
    "    actor.load_state_dict(state_dict[\"model_state_dict\"])\n",
    "else:\n",
    "    actor.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "# Get the embeddings and domain labels\n",
    "embeddings, domain_labels = get_embeddings_and_domain_labels(\n",
    "    actor, trainloader, sample_size=sample_size\n",
    ")\n",
    "\n",
    "# Visualize the embeddings using t-SNE and PCA\n",
    "title = \"Co-training, naive mixing\"\n",
    "visualize_embeddings_tsne(embeddings, domain_labels, title=title)\n",
    "visualize_embeddings_pca(embeddings, domain_labels, title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot embeddings for co-trained model with up-weighting of real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get weights\n",
    "_, wts = get_model_from_api_or_cached(run_id_upwt, \"latest\", wandb_mode=\"online\")\n",
    "\n",
    "# Load the weights into the actor\n",
    "state_dict = torch.load(wts)\n",
    "if \"model_state_dict\" in state_dict:\n",
    "    actor.load_state_dict(state_dict[\"model_state_dict\"])\n",
    "else:\n",
    "    actor.load_state_dict(state_dict)\n",
    "\n",
    "# Get the embeddings and domain labels\n",
    "embeddings, domain_labels = get_embeddings_and_domain_labels(\n",
    "    actor, trainloader, sample_size=sample_size\n",
    ")\n",
    "\n",
    "# Visualize the embeddings using t-SNE and PCA\n",
    "title = \"Co-training, confusion loss $\\\\lambda=10^{-4}$\"\n",
    "visualize_embeddings_tsne(embeddings, domain_labels, title=title)\n",
    "visualize_embeddings_pca(embeddings, domain_labels, title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot embeddings for co-trained model with confusion loss and up-weighting of real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get weights\n",
    "import math\n",
    "\n",
    "\n",
    "cfg, wts = get_model_from_api_or_cached(run_id_conf, \"latest\", wandb_mode=\"online\")\n",
    "\n",
    "# Load the weights into the actor\n",
    "state_dict = torch.load(wts)\n",
    "if \"model_state_dict\" in state_dict:\n",
    "    actor.load_state_dict(state_dict[\"model_state_dict\"])\n",
    "else:\n",
    "    actor.load_state_dict(state_dict)\n",
    "\n",
    "# Get the embeddings and domain labels\n",
    "embeddings, domain_labels = get_embeddings_and_domain_labels(\n",
    "    actor, trainloader, sample_size=sample_size\n",
    ")\n",
    "\n",
    "confusion = cfg.actor.confusion_loss_beta\n",
    "\n",
    "# Visualize the embeddings using t-SNE and PCA\n",
    "title = f\"Co-training, confusion loss $\\\\lambda=10^{int(math.log10(float(confusion)))}$\"\n",
    "visualize_embeddings_tsne(embeddings, domain_labels, title=title)\n",
    "visualize_embeddings_pca(embeddings, domain_labels, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get weights\n",
    "_, wts = get_model_from_api_or_cached(run_id_confusion2, \"latest\", wandb_mode=\"online\")\n",
    "\n",
    "# Load the weights into the actor\n",
    "state_dict = torch.load(wts)\n",
    "if \"model_state_dict\" in state_dict:\n",
    "    state_dict = state_dict[\"model_state_dict\"]\n",
    "\n",
    "if \"model._dummy_variable\" in state_dict:\n",
    "    del state_dict[\"model._dummy_variable\"]\n",
    "\n",
    "actor.load_state_dict(state_dict)\n",
    "\n",
    "# Get the embeddings and domain labels\n",
    "embeddings, domain_labels = get_embeddings_and_domain_labels(\n",
    "    actor, trainloader, sample_size=sample_size\n",
    ")\n",
    "\n",
    "# Visualize the embeddings using t-SNE and PCA\n",
    "title = \"Co-training, confusion loss $\\\\lambda=10^{-2}$\"\n",
    "visualize_embeddings_tsne(embeddings, domain_labels, title=title)\n",
    "visualize_embeddings_pca(embeddings, domain_labels, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
